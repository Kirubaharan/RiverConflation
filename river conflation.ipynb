{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired river network conflation\n",
    "\n",
    "Please reference this publication whenever this code is used\n",
    "http://geomorphometry.org/system/files/Read2011geomorphometry.pdf\n",
    "Thanks\n",
    "\n",
    "Takes about 25 minutes to read in the entire geofabric.\n",
    "Takes less then 5 minutes to run the analysis for Tasmania (17k stream links).\n",
    "\n",
    "\n",
    "Is not memory /time efficent yet when scaling to the entire geofabric dataset (1.5 millon stream links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netGDB = r'C:\\data\\temp\\SH_Network_GDB_V2_1_1\\SH_Network_GDB\\SH_Network.gdb'\n",
    "pkl_path = r'C:\\data\\temp'\n",
    "#checks all nodes within at least this distance for a conflation\n",
    "#its a time saving optimisation and should be set on the large size to avoid missing the best result\n",
    "#A guide to setting this is: set it a bit bigger then the largest expected separation of conflated nodes\n",
    "#for small networks networks just set it very large\n",
    "#in projection units (1degree approx 100km)\n",
    "searchRadius = 0.1\n",
    "#number of potential matches to keep for each feature\n",
    "maxMatchKeep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.prepared import prep\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import mapping,Polygon,Point,LineString\n",
    "import fiona\n",
    "from rtree import index\n",
    "import networkx as nx\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step is to produce a networkx multiDiGraph from your source data.\n",
    "\n",
    "The code below does that specifically for the Australian geofabric dataset in ESRI geodatabase format\n",
    "\n",
    "(note the geodatabase has to be upgraded in ESRI software first so that gdal can read it).\n",
    "\n",
    "For other datasets it would need to be changed to produce the same output from the different inputs.\n",
    "\n",
    "This needs to be done for both the conflation source and destination networks\n",
    "\n",
    "the key thing is some sort of from and to node attribution to build the network structure between subcatchments. if this isnt available it can be generated from the coords of the stream links. Code for this not here but available on request.\n",
    "\n",
    "subcatchments and streams need a one to one relationship. streams without a catchment are (i think) fully handled. catchments without streams are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_geofabric_catch_duplicates(DG):\n",
    "    '''deal with special case of duplicate catchment polygons in the geofabric\n",
    "    \n",
    "    Im not sure why the geofabric allows multiple stream features to \n",
    "    have a relationship to the same subcatchment.\n",
    "    Maybe it is ok and i just need to change my expected data model..... will have to think about that.\n",
    "    Its a bit inconvinient though. \n",
    "    Is there a good way to weed these out logically and consistently?\n",
    "    Here is my attempt. Based on the fact that they seem to occur only at flow splits.\n",
    "    '''\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update([(data['cid'])])\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            if data['cid'] in new_set:\n",
    "                data['cid'] = None\n",
    "                data['subCatch'] = Polygon()\n",
    "    \n",
    "    #unfortunatly many cases still remain\n",
    "    #simple way of handling them is to make an arbitary choice\n",
    "    #first in keeps the catchment\n",
    "    #not ideal\n",
    "    ss = set()\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        if data['cid'] is not None:\n",
    "            if data['cid'] in ss:\n",
    "                print 'WARNING: duplicate catchment removed' + str(data['cid'])\n",
    "                data['cid'] = None\n",
    "                data['subCatch'] = Polygon()\n",
    "            #assert data['cid'] not in ss, 'duplicate catchment id ' + str(data['cid'])\n",
    "            ss.add(data['cid'])\n",
    "\n",
    "def read_geofabric_data(netGDB):\n",
    "    catch = {}\n",
    "    with fiona.open(netGDB, layer='AHGFCatchment') as c:\n",
    "        for feat in c:\n",
    "            geom = shape(feat['geometry'])\n",
    "            cid = feat['properties']['HydroID']\n",
    "            assert cid not in catch #shouldnt be duplicates \n",
    "            catch[cid] = geom\n",
    "    \n",
    "    DG=nx.MultiDiGraph()\n",
    "    with fiona.open(netGDB, layer='AHGFNetworkStream') as c:\n",
    "        for feat in c:\n",
    "            streamLink = shape(feat['geometry'])\n",
    "             #for some reason these are coming in as multipart features with only one part - no need for this\n",
    "            assert streamLink.type == 'MultiLineString'\n",
    "            assert len(streamLink.geoms) == 1\n",
    "            streamLink = streamLink.geoms[0]\n",
    "            \n",
    "            ##remove this - just here for testing\n",
    "            if streamLink.representative_point().y > -40.5: #tasmania for testing\n",
    "                continue\n",
    "            \n",
    "            sid = feat['properties']['HydroID']\n",
    "            cid = feat['properties']['DrainID']\n",
    "            fid = feat['properties']['From_Node']\n",
    "            tid = feat['properties']['To_Node']\n",
    "            subCatch = catch.get(cid,Polygon())\n",
    "            DG.add_edge(fid, tid, id=sid,cid=cid,subCatch=subCatch,stream=streamLink)\n",
    "            \n",
    "    return DG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DG1 = read_geofabric_data(netGDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_geofabric_catch_duplicates(DG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gpickle(DG1, os.path.join(pkl_path, 'tas_conflation.p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing im just working with one network and shifting it spatially a bit.\n",
    "\n",
    "it wont show the full capabilities of the tool but it is a useful test case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TESTING ONLY now shift the geometries a bit in the copy just to make it a useful test\n",
    "DG2 = DG1.copy()\n",
    "#assumes geographic coords\n",
    "from shapely.ops import transform\n",
    "for f,t,k,data in DG2.edges_iter(data=True,keys=True):\n",
    "    data['subCatch'] = transform(lambda x, y, z=None: (x+0.01, y+0.01), data['subCatch'])\n",
    "    data['stream'] =  transform(lambda x, y, z=None: (x+0.01, y+0.01), data['stream'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gpickle(DG2, os.path.join(pkl_path, 'tas_conflation2.p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can start from here by loading in the pickles that were created with the above code earlier.\n",
    "Run the imports and global variables code at the top first though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_index(DG):\n",
    "    '''build spatial and other indexes'''\n",
    "    #remember rtrees are not picklable - doh\n",
    "    DG_idx = index.Index()\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        if not data['subCatch'].is_empty:\n",
    "            DG_idx.insert(0, data['subCatch'].bounds,obj=(f,t,k))\n",
    "    return DG_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upstream_edge_set(DG):\n",
    "    '''build up a list of upstream edge ids in the destination network'''\n",
    "    #wonder it if these lists become unwieldy for edges at the root of a big tree\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update((data['ids']))\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            data['ids'] = new_set.union([(f,t,k)])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def catch_area(DG):\n",
    "    '''calc catchment area\n",
    "    \n",
    "    works with anabranching networks without double counting'''\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        catchArea = 0.0\n",
    "        for e in data['ids']:\n",
    "            catchArea += DG.get_edge_data(*e)['subCatch'].area\n",
    "        data['catchArea'] = catchArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_overlaps(DG1,DG2,DG2_idx):\n",
    "    '''build up dictionary of overlapping areas between 2 graphs'''\n",
    "\n",
    "    for f,t,data in DG1.edges_iter(data=True):\n",
    "        geom = data['subCatch']\n",
    "        data['overlaps'] = {}\n",
    "        if not geom.is_empty:\n",
    "            prepGeom = prep(geom)\n",
    "            for e in DG2_idx.intersection(geom.bounds,objects='raw'):\n",
    "                nGeom = DG2.get_edge_data(*e)['subCatch']\n",
    "                if prepGeom.intersects(nGeom):\n",
    "                    area = geom.intersection(nGeom).area\n",
    "                    if area > 0.0:\n",
    "                        data['overlaps'][e] = area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_full_overlaps(DG):\n",
    "    '''build up table of overlapping area for the entire catchment above and including it\n",
    "\n",
    "    assumes build_overlaps has been run with DG as the first network\n",
    "    works with anabranching networks without double counting\n",
    "    '''\n",
    "\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        fullOverlaps = collections.defaultdict(float)\n",
    "        for e in data['ids']:\n",
    "            for k, v in DG.get_edge_data(*e)['overlaps'].iteritems():\n",
    "                fullOverlaps[k] += v\n",
    "        data['fullOverlaps'] = fullOverlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTE:upstream_edge_set,catch_area,build_overlaps, and build_full_overlaps create and store a lot of data\n",
    "#for DG1 these could be chained together as the outer loop of find_all_matches to avoid keeping and of this intermediate data\n",
    "#big saving on memory footprint possible for large DG1 networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets run some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DG1 = nx.read_gpickle(os.path.join(pkl_path, 'tas_conflation.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DG2 = nx.read_gpickle(os.path.join(pkl_path, 'tas_conflation2.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DG2_idx = build_index(DG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upstream_edge_set(DG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upstream_edge_set(DG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catch_area(DG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catch_area(DG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_overlaps(DG1,DG2,DG2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_full_overlaps(DG1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find sum up areas to find the catchment overlap for every combination. We are only interested in the best or a short list of the overlaps that match well\n",
    "The simple approach is a brute force exhustive test of all combinations. This works well for a few thousand (75 minutes for 17k x 17k) sub catchments in each graph. This would not scale well as network sizes increase.\n",
    "\n",
    "There are a few ways to reduce the set of catchments to test for a match. One issue to keep in mind is to not make assumptions about how similar the two networks topology might be. This makes learning from nearby matches problematic although possible. limiting the search based on area similarity or stopping it based on an analysis of if any other catchments are likely to be a match is also possible.\n",
    "\n",
    "However, To keep the code as simple and readable these were avoided in favour of a technique that was simple and effective. The approach taken is to use a tunable spatial proximity limit that should be set to a size that is expected to ensure finding the best matches within that radius. setting it too small would cause missed matches, too large would just take longer. This is the only downside of this method. I would have prefered to avoided confusing users with parameters to tune. Generally i would suggest setting if fairly large (10km?) to avoid issues but still get a reasonable speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_all_matches(DG1,DG2,DG2_idx,searchRadius,maxMatchKeep=1):\n",
    "    '''for each catchment in DG1 find list the best matches\n",
    "    \n",
    "    This is a good and fairly efficent way of doing this.\n",
    "    In a large network if you are only interested in a few catchment\n",
    "    then you could write a similar routine that only ran for those catchments to save some time.\n",
    "    Here the search is limited to nearby catchments by searchRadius\n",
    "    This could easily be modified to limit in different ways\n",
    "    for instance first order stream have no need to be searched beyond their own bounding box\n",
    "    \n",
    "    maxMatchKeep\n",
    "    limits the number of potential matches that are kept to a short list of n items.\n",
    "    The best items are chosen using a simple test of the quality of the match\n",
    "    This is used to reduce the memory footprint of the returned dictionary\n",
    "    '''\n",
    "    matches = {}\n",
    "    for f,t,k,data in DG1.edges_iter(data=True,keys=True):\n",
    "        if len(data['fullOverlaps']) == 0:\n",
    "            continue\n",
    "        match = []\n",
    "        searchBounds = Point(data['stream'].coords[-1]).buffer(searchRadius).bounds\n",
    "        for e in DG2_idx.intersection(searchBounds,objects='raw'):\n",
    "            data2 = DG2.get_edge_data(*e)\n",
    "            overlap = sum( data['fullOverlaps'].get(k,0) for k in data2['ids'])\n",
    "            if overlap > 0.0:\n",
    "                qual = (overlap+overlap)/(data['catchArea']+data2['catchArea'])\n",
    "                match.append((qual,overlap,e))\n",
    "        #just keep the best few matches\n",
    "        match.sort(reverse=True)\n",
    "        matches[(f,t,k)] = match[0:maxMatchKeep]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = find_all_matches(DG1,DG2,DG2_idx,searchRadius,maxMatchKeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process the features being matched together are the ends of a subcatchment. Just before a confluence, or the next reach. Nodes, if you like, but considering each inflow to a node seperatly.\n",
    "\n",
    "Once you have this shortlist of matches for each feature you need to consider way to summarise that data.\n",
    "\n",
    "\n",
    "The most basic option is to just take the best match for each feature. This gives you a node to node relationship (although considering each inflow to a node seperatly).\n",
    "\n",
    "A useful way to build on that is to consider each incoming branch at a confluence together so that these confluences are accuratly identified. Wtihout this small differences in the very end of a catchment can see the best match shifting upstream a stream link or 2 to minimise the errors. This would give you a true node to node relationship.\n",
    "\n",
    "Some uses might want to draw on additional attribute data to pick from the best matches. If this is required then in is probably because the subcatchment of the feature being conflated isnt significant enough to be well captured accuratly by one of the dems. nevertheless this is still possible but is left as additional work for the end user. The table of possible matches is available for this analysis.\n",
    "\n",
    "Once a set of 'best' matches for points in the network has been selected these can then be expanded out to provide a conflation of reaches and subcatchments. \n",
    "\n",
    "You need to consider that the level of detail between the 2 networks may be very different and the topology may be very different.Any anabranching in the network also needs careful consideration. The differences can result is multiple features conflating to the same point, confluences being seperated or in a different order. These are generally going to be differences in the 2 networks not issues with the results from this tool. Ideally this tool can help highlight these differences.\n",
    "\n",
    "Using these tools you are able to transfer attribute data between networks. Additionally you can start to make assessments of the level of detail that is supported by the DEM and the comparative qualities of the networks and the source DEMs.\n",
    "\n",
    "Below are some simple tools for doing some of what has been described above. It is by no means an exhaustive set of tools and is subject to ongoing work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = [(k,l[0][2],l[0][0]) for k,l in matches.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_debug_lines(DG1,DG2,best,fileName):\n",
    "    '''a simple output to show how each to node matches up\n",
    "    \n",
    "    handy for looking for errors in the conflation'''\n",
    "    schema = {\n",
    "        'geometry': 'LineString',\n",
    "        'properties': {'qual': 'float'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for e1,e2,qual in best:\n",
    "            p1 = DG1.get_edge_data(*e1)['stream'].coords[-1]\n",
    "            p2 = DG2.get_edge_data(*e2)['stream'].coords[-1]\n",
    "            geom = LineString(LineString([p1, p2]))\n",
    "            c.write({\n",
    "                'geometry': mapping(geom),\n",
    "                'properties': {'qual': qual},\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_debug_lines(DG1,DG2,best,os.path.join(pkl_path, 'debug_lines.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: not finished the reimplementation\n",
    "\n",
    "TODO\n",
    "\n",
    "unify the matches at a confluence to one match. difference between these and subcatchment match is also useful.\n",
    "\n",
    "transfer/rewrite code that maps the conflation out along reaches and subcatchments\n",
    "\n",
    "transfer/rewrite code that highlights topology differences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_catch(DG,fileName):\n",
    "    schema = {\n",
    "        'geometry': 'Polygon',\n",
    "        'properties': {'id': 'int'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for f,t,data in DG.edges_iter(data=True):\n",
    "            if not data['subCatch'].is_empty:\n",
    "                c.write({\n",
    "                    'geometry': mapping(data['subCatch']),\n",
    "                    'properties': {'id': data['id']},\n",
    "                })\n",
    "\n",
    "def write_stream(DG,fileName):\n",
    "    schema = {\n",
    "        'geometry': 'LineString',\n",
    "        'properties': {'id': 'int'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for f,t,data in DG.edges_iter(data=True):\n",
    "            if not data['stream'].is_empty:\n",
    "                c.write({\n",
    "                    'geometry': mapping(data['stream']),\n",
    "                    'properties': {'id': data['id']},\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_catch(DG1,os.path.join(pkl_path, 'catch1.shp'))\n",
    "write_catch(DG2,os.path.join(pkl_path, 'catch2.shp'))\n",
    "write_stream(DG1,os.path.join(pkl_path, 'stream1.shp'))\n",
    "write_stream(DG2,os.path.join(pkl_path, 'stream2.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some code used for error testing in development\n",
    "#assumes full overlap\n",
    "#will get some show up due to floating point issues\n",
    "def test_stuff(DG):\n",
    "    for f,t,data in DG.edges_iter(data=True):\n",
    "        area = data['subCatch'].area\n",
    "        area2 = 0.0\n",
    "        for k, v in data['overlaps'].iteritems():\n",
    "                area2 += v\n",
    "        if (abs(area - area2) > 0.000000000000000001):\n",
    "            print area,area2\n",
    "\n",
    "    #a test for errors\n",
    "    for f,t,data in DG.edges_iter(data=True):\n",
    "        area = data['catchArea']\n",
    "        area2 = 0.0\n",
    "        for k, v in data['fullOverlaps'].iteritems():\n",
    "                area2 += v\n",
    "        if (abs(area - area2) > 0.00000000000001):\n",
    "            print area,area2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
